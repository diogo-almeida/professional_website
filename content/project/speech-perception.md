+++
# Date this page was created.
date = 2018-08-26T00:00:00

# Project title.
title = "Speech Perception"

# Project summary to display on homepage.
summary = ""
#"Lorem ipsum dolor sit amet, consectetur adipiscing elit."

# Optional image to display on homepage (relative to `static/img/` folder).
image_preview = ""
#"experimental-syntax.jpg"

# Tags: can be used for filtering projects.
# Example: `tags = ["machine-learning", "deep-learning"]`
tags = ["Speech Perception"]

# Optional external URL for project (replaces project detail page).
external_link = ""

# Does the project detail page use math formatting?
math = false

# Optional featured image (relative to `static/img/` folder).
[header]
image = ""
#"headers/experimental-syntax-wide.jpg"
caption = ""
#"Photo by Min An from Pexels"

+++

Central to language comprehension is how the long-term memory representations of linguistic information guide or otherwise impinge on perception. I have previously approached this in the domain of vision and sign language (Almeida, Poeppel & Corina, 2016), but my current focus is in speech perception (Schluter, Politzer-Ahles & Almeida, 2016; Schluter, Politzer-Ahles, Al Kaabi & Almeida, 2017; Politzer-Ahles, Schluter, Wu & Almeida, 2016). In particular, I am interested in the extent to which the representations of speech sounds in long-term memory is tied to sensory processing: do speech sound categories in long-term memory retain fine-grained acoustic/phonetic information or are they fairly abstract and/or optimally sparse?

Research from my lab on this topic has exploited an automatic difference-detection brain response in the auditory domain (the Mismatch Negativity - MMN), and the results have consistently pointed to long-term memory representations for speech sounds that are fairly abstract when it comes to sensory information, and also potentially sparse in their featural content.
